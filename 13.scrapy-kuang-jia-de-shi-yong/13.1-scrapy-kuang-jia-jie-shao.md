### 1.架构

![](/assets/spider-14.14.1-1.png)

scrapy架构可分为如下部分:

* Engine 引擎，处理整个系统的数据流处理、触发事务，是整个框架的核心
* Item 项目，它定义了爬取结果的数据结构，爬取的数据会被赋值成该 Item 对象
* Scheduler 调度器，接受引擎发过来的请求并将其 列中 在引擎再次请求的时候将请求提供给引擎
* Down loader 下载器，下载网页内容，并将网页 容返回给蜘蛛
* Spiders 蜘蛛，其内定义了爬取的逻辑和网页 解析规 ，它主要负责解析响应并生成提取结果和新的请求
* Item Pipel ne 项目管道，负责处理由蜘蛛从网页中 取的项目，它的主要任务是清洗、验证和存储数据
* Downloader Middlewares 下载器中间件，位于引擎和下载器之 的钩子框架，主要处理引擎与下载器之间的请求及响应
* Spide Middlewares 蜘蛛中间件，位于引擎和蜘蛛之 的钩子框架，主要处理蜘蛛输入的响应和输出的结果及新的请求

### 2.数据流

Scrapy 中的数据流由引擎控制，数据流的过程如下

* \(1\) Engin 首先打开一个网站，找到处理该网站的 Spider ，并向该 Spider 请求第一个要爬取的 URL
* \(2\) in er 中获取到第一个要爬取的 URL ，并通过 dul ques 的形式调度
* \(3\) gi Sc dul 请求下一个要爬取的 URL
* \(4\) Sc duler 返回下一个要爬取的 URL in , ngin URL 通过 Downlo der MiddJewares发给 Down oa er 下载

* \(5 ）一旦页面下载完毕， ow oa 生成该页面的 Re pon ，并将其通过 wnload Middlewares发送给 in

* \(6\) in 从下载器中接收到 espon se ，并将其通过 Spider Middleware 发送给 Spid 处理

* \(7\) pid er 处理 es ponse ，并返回爬取到的 It 及新的 Reque ngin
* \(8\) ine pid er 返回的 It It Pip line ，将新 Requ st Schedul
* \(9）重复第（ 2）步到第（8 ）步，直到 Sc dule 中没有更多的 que ngine 关闭该网站，爬取结束

### 3.项目结构

Scrapy 框架和pyspider不同，它是通过命令行来 建项目的，代码 编写还是需要 IDE, 项目

建之后，项目文件结构如下所示

* scrapy .cfg
  * project/
    * init.py
    * items.py
    * pipelines.py 
    * settings.py
    * middlewares .py
    * spiders/
      * init .py
      * spiderl. py
      * spider2 .py

这里各个文件的功能描述如下:

* scrapy.cfg: 它是 Scrapy 项目的配置文件，其内定义了项目的配置文件路径 部署相关信息等内容
* items.py :它定义 Item 数据结构，所有的 Item 的定义都可以放这里
* pipelines.py: 它定义 Item Pipel ine 的实现，所有的 Item Pipeline 实现都可以放这里
* settings.py: 它定义项目的全局配置
* middlewares.py: 它定义 Spi der Middl wares Downloader Middlewares
* spiders: 其内包含 个个 Spide 的实现，每个 Spider 都有一 个文件




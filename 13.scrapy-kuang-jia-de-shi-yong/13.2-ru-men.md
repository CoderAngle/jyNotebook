### 1.流程

本节要完成的任务:

* 创建Scrapy 项目
* 创建Spider 来抓取站点和处理数据
* 通过命令行将抓取的内容导出
* 将抓取的内容保存的到 MongoDB 数据库

### 2.创建项目

* 创建 Scrapy 项目，项目文件可以直接用 scrapy 命令生成

```
scrapy startproject myproject
```

这个命令可以在任意文件夹运行 如果提示权限问题，可以加 sudo 运行该命令，这个命令将会创 个名为 tutorial 文件夹，文件夹结构如下所示：

```
scrapy. cfg # Scrapy 部署时的配置文件
tutorial ＃项目的模块 需要从这里引入
_init_.PY
items.py # Items 的定义，定义爬取的数据结构
middlewares.py # Middlewares 的定义，定义爬取时的中间件
pipelines.py # Pipelines 的定义，定义数据管道
settings.py ＃配置文件
spiders ＃放置 Spiders 的文件夹
_init_ py
```

### 3.创建Spider

 Spider 是向己定义的类， Scrapy 用它来从网页里抓取内容，并解析抓取的结果 不过这个类必须 继承 Scrapy 提供的 Spider类 scrapy.Spider ，还要定义 Spider 的名称和起始请求，以及怎样处理爬取后的结果的方法

 也可以使用命令行创建一个 Spider 比如要生成 app这个 Spider ，可以执行如下命令：

```
cd myproject
scrapy genspider baidu www.baidu.com
```

进入刚才创建的 myproject文件夹，然后执行 genspider 命令 第一个参数是 Spider 名称，第二

个参数是网站域名 执行完毕之后， spiders 文件夹中多了一个 baidu.py ，它就是刚刚创建的 Spider,

内容如下所示：

```
# -*- coding: utf-8 -*-
import scrapy


class BaiduSpider(scrapy.Spider):
    name = 'baidu'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/']

    def parse(self, response):
        pass

```

这里有 个属性－name、allowed\_domains 和 start\_urls ，还有一个方法 parse

* name ，它是每个项目唯一的名字，用来区分不同的 Spider
* allowed domains ，它是允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉
* start\_urls ，它包含了 Spider 在启动时爬取的 url 列表，初始请求是由它来定义的
* parse ，它是 Spider 一个方法 默认情况下，被调用时 start\_urls 里面的链接构成的请求完成下载执行后，返回的响应就会作为唯一的参数传递给这个函数 该方法负责解析返回的响应、提取数据或者进一步生成要处理的请求

### 4.创建项目

Item 是保存爬取数据的容器，它的使用方法和字典类似 不过，相比字典， Item 多了额外的保护机制，可以避免拼写错误或者定义字段错误

创建 Item 需要继承 scrapy.Item 类，并且定义类型为 scrapy.Field 字段，观察目标网站，可以获取到到内容有 text author tags

定义 Item ，此时将 items.py 修改如下：

```
import scrapy
class Quoteitem(scrapy. Item):
    text = scrapy . Field()
    author = scrapy. Field()
    tags = scrapy . Field()
```



